# =============================================================================
# Agentic RAG System - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values.
# All settings can also be configured via the web UI Settings page.
#
# Feature #322: Centralized environment-based configuration
# =============================================================================


# =============================================================================
# ENVIRONMENT
# =============================================================================
# Environment mode: development, production, or testing
# Default: development
ENVIRONMENT=development

# Enable debug mode (shows detailed errors, enables hot-reload)
# Set to false in production!
# Default: true (development), false (production)
DEBUG=true


# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# PostgreSQL connection URLs (both async and sync versions required)
# Format: postgresql+asyncpg://USER:PASSWORD@HOST:PORT/DATABASE
# Format: postgresql://USER:PASSWORD@HOST:PORT/DATABASE
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/agentic_rag
DATABASE_SYNC_URL=postgresql://postgres:postgres@localhost:5432/agentic_rag

# Connection pool settings
# DATABASE_POOL_SIZE: Number of connections to keep open (default: 5)
# DATABASE_MAX_OVERFLOW: Additional connections allowed when pool is full (default: 10)
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10


# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
# Host to bind to (0.0.0.0 allows external connections)
# Default: 0.0.0.0
HOST=0.0.0.0

# Port to listen on
# Default: 8000
PORT=8000

# Number of worker processes (only used in production with DEBUG=false)
# Default: 1
WORKERS=1


# =============================================================================
# CORS CONFIGURATION
# =============================================================================
# Allowed CORS origins (comma-separated list)
# Leave empty to use default development origins (localhost:3000-3020)
# Example: CORS_ORIGINS=https://myapp.com,https://api.myapp.com
CORS_ORIGINS=


# =============================================================================
# API KEYS (Sensitive - NEVER commit actual values!)
# =============================================================================
# These can also be configured via the Settings UI
# Environment variables take precedence over UI settings

# OpenAI API Key (required for GPT models and OpenAI embeddings)
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Cohere API Key (optional - for re-ranking)
# Get your key at: https://dashboard.cohere.com/api-keys
COHERE_API_KEY=

# OpenRouter API Key (optional - alternative LLM provider)
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=


# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Default LLM model for chat responses
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, or ollama:model-name
# Default: gpt-4o
DEFAULT_LLM_MODEL=gpt-4o

# Default embedding model for vector search
# Options: text-embedding-3-small, text-embedding-3-large, or ollama:model-name
# Default: text-embedding-3-small
DEFAULT_EMBEDDING_MODEL=text-embedding-3-small

# Default reranker for search results
# Options: cohere (cloud API), local (offline CrossEncoder)
# Default: cohere
DEFAULT_RERANKER=cohere

# CrossEncoder model for local reranking (used when DEFAULT_RERANKER=local)
# Options: Any HuggingFace CrossEncoder model
# Popular models:
#   - cross-encoder/ms-marco-MiniLM-L-6-v2 (fast, good quality)
#   - cross-encoder/mmarco-mMiniLMv2-L12-H384-v1 (multilingual)
#   - cross-encoder/qnli-distilroberta-base (QNLI trained)
# Default: cross-encoder/ms-marco-MiniLM-L-6-v2
# Note: Model auto-downloads on first use (~50MB for default model)
RERANKER_CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2


# =============================================================================
# OLLAMA CONFIGURATION
# =============================================================================
# Base URL for Ollama API (if using local models)
# Default: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434


# =============================================================================
# LLAMA.CPP CONFIGURATION (llama-server)
# =============================================================================
# Base URL for llama-server (llama.cpp HTTP server with OpenAI-compatible API)
# llama-server provides local LLM inference using GGUF models with Metal/CUDA acceleration.
# Start it with: llama-server -m model.gguf --port 8080
# Default: http://localhost:8080
LLAMACPP_BASE_URL=http://localhost:8080


# =============================================================================
# MLX CONFIGURATION (mlx_lm.server - macOS Apple Silicon only)
# =============================================================================
# Base URL for MLX server (Apple's ML framework for Apple Silicon)
# mlx_lm.server provides local LLM inference using HuggingFace models with native Metal acceleration.
# Start it with: python -m mlx_lm.server --model mlx-community/Qwen2.5-32B-Instruct-4bit --port 8081
# Default: http://localhost:8081
MLX_BASE_URL=http://localhost:8081


# =============================================================================
# FILE UPLOAD CONFIGURATION
# =============================================================================
# Maximum file size for uploads in megabytes
# Default: 100
MAX_FILE_SIZE_MB=100

# Directory for storing uploaded files (relative to backend or absolute path)
# Default: ./uploads
UPLOAD_DIR=./uploads


# =============================================================================
# BACKUP CONFIGURATION
# =============================================================================
# Directory for automatic backups (relative to backend or absolute path)
# Default: ./automatic_backups
BACKUPS_DIR=./automatic_backups

# Number of daily backups to retain
# Default: 7
BACKUP_RETENTION_DAYS=7

# Number of weekly backups to retain
# Default: 4
WEEKLY_BACKUP_RETENTION=4


# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
LOG_LEVEL=INFO

# Log format: json (for production/structured logging) or text (for development)
# Default: json
LOG_FORMAT=json

# Enable file logging (in addition to console)
# Default: true
LOG_TO_FILE=true

# Directory for log files (relative to backend or absolute path)
# Default: ./logs
LOG_DIR=./logs


# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================
# Secret key for encryption (API keys stored in settings)
# IMPORTANT: Generate a new key for production!
# Generate with: python -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=dev-secret-key-change-in-production


# =============================================================================
# TELEGRAM CONFIGURATION (Optional)
# =============================================================================
# Bot token from @BotFather
TELEGRAM_BOT_TOKEN=

# Webhook URL for receiving updates (requires public HTTPS URL)
TELEGRAM_WEBHOOK_URL=


# =============================================================================
# TWILIO/WHATSAPP CONFIGURATION (Optional)
# =============================================================================
# Twilio Account SID from https://console.twilio.com
TWILIO_ACCOUNT_SID=

# Twilio Auth Token
TWILIO_AUTH_TOKEN=

# WhatsApp number in format: whatsapp:+1234567890
TWILIO_WHATSAPP_NUMBER=


# =============================================================================
# RAG CONFIGURATION
# =============================================================================
# Number of chunks to retrieve from vector search (5-100)
# Default: 10
DEFAULT_TOP_K=10

# Minimum relevance score for normal mode (0.0-1.0)
# Default: 0.4
MIN_RELEVANCE_THRESHOLD=0.4

# Minimum relevance score for strict mode (0.0-1.0)
# Default: 0.6
STRICT_RELEVANCE_THRESHOLD=0.6


# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================
# Threshold in milliseconds for logging slow requests
# Default: 1000.0 (1 second)
SLOW_REQUEST_THRESHOLD_MS=1000.0


# =============================================================================
# RATE LIMITING CONFIGURATION (Feature #324)
# =============================================================================
# Enable/disable rate limiting (recommended: true in production)
# Default: true
RATE_LIMIT_ENABLED=true

# Default rate limit for general API endpoints
# Format: {count}/{period} where period is second, minute, hour, day
# Default: 100/minute
RATE_LIMIT_DEFAULT=100/minute

# Rate limit for chat endpoints (lower due to LLM costs)
# Default: 60/minute
RATE_LIMIT_CHAT=60/minute

# Rate limit for upload endpoints (lower due to processing costs)
# Default: 10/minute
RATE_LIMIT_UPLOAD=10/minute

# Rate limit for health check endpoints (higher for monitoring)
# Default: 300/minute
RATE_LIMIT_HEALTH=300/minute

# Comma-separated list of IPs to whitelist from rate limiting
# These IPs will not be subject to rate limits
# Default: 127.0.0.1,localhost
RATE_LIMIT_WHITELIST=127.0.0.1,localhost
