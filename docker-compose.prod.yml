# =============================================================================
# Agentic RAG System - Production Docker Compose Configuration
# =============================================================================
# This file defines the production deployment setup with:
# - PostgreSQL with pgvector extension
# - Backend API service (FastAPI)
# - Frontend with nginx reverse proxy
# - Optional Ollama service for local LLM inference
#
# Usage:
#   docker compose -f docker-compose.prod.yml up -d
#
# With Ollama:
#   docker compose -f docker-compose.prod.yml --profile ollama up -d
# =============================================================================

name: agentic-rag

services:
  # ===========================================================================
  # PostgreSQL Database with pgvector
  # ===========================================================================
  postgres:
    image: pgvector/pgvector:pg16
    container_name: agentic-rag-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-agentic_rag}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-agentic_rag}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - rag-network

  # ===========================================================================
  # Backend API Service (FastAPI)
  # ===========================================================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: agentic-rag-backend
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Environment
      ENVIRONMENT: production
      DEBUG: "false"

      # Database
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-agentic_rag}
      DATABASE_SYNC_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-agentic_rag}
      DATABASE_POOL_SIZE: ${DATABASE_POOL_SIZE:-5}
      DATABASE_MAX_OVERFLOW: ${DATABASE_MAX_OVERFLOW:-10}

      # Server
      HOST: "0.0.0.0"
      PORT: "8000"
      WORKERS: ${BACKEND_WORKERS:-2}

      # CORS - Allow frontend
      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost,http://localhost:80}

      # API Keys (from host environment or .env file)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      COHERE_API_KEY: ${COHERE_API_KEY:-}
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}

      # Model Configuration
      DEFAULT_LLM_MODEL: ${DEFAULT_LLM_MODEL:-gpt-4o}
      DEFAULT_EMBEDDING_MODEL: ${DEFAULT_EMBEDDING_MODEL:-text-embedding-3-small}
      DEFAULT_RERANKER: ${DEFAULT_RERANKER:-cohere}

      # Ollama (points to optional Ollama service)
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}

      # llama.cpp server (points to optional llama-server service)
      LLAMACPP_BASE_URL: ${LLAMACPP_BASE_URL:-http://llamacpp:8080}

      # MLX server (macOS only - Apple Silicon local inference)
      MLX_BASE_URL: ${MLX_BASE_URL:-http://host.docker.internal:8081}

      # File Upload
      MAX_FILE_SIZE_MB: ${MAX_FILE_SIZE_MB:-100}
      UPLOAD_DIR: /app/uploads
      BACKUPS_DIR: /app/automatic_backups

      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: json
      LOG_TO_FILE: "true"
      LOG_DIR: /app/logs

      # Security
      SECRET_KEY: ${SECRET_KEY:-change-me-in-production}

      # Telegram (optional)
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
      TELEGRAM_WEBHOOK_URL: ${TELEGRAM_WEBHOOK_URL:-}

      # Twilio/WhatsApp (optional)
      TWILIO_ACCOUNT_SID: ${TWILIO_ACCOUNT_SID:-}
      TWILIO_AUTH_TOKEN: ${TWILIO_AUTH_TOKEN:-}
      TWILIO_WHATSAPP_NUMBER: ${TWILIO_WHATSAPP_NUMBER:-}

      # RAG Configuration
      DEFAULT_TOP_K: ${DEFAULT_TOP_K:-10}
      MIN_RELEVANCE_THRESHOLD: ${MIN_RELEVANCE_THRESHOLD:-0.4}
      STRICT_RELEVANCE_THRESHOLD: ${STRICT_RELEVANCE_THRESHOLD:-0.6}

    volumes:
      # Persistent storage for uploads, logs, and backups
      - backend_uploads:/app/uploads
      - backend_logs:/app/logs
      - backend_backups:/app/automatic_backups
      # BM25 index persistence
      - backend_bm25:/app/bm25_index.pkl
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - rag-network

  # ===========================================================================
  # Frontend with Nginx Reverse Proxy
  # ===========================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_API_URL: /api
    container_name: agentic-rag-frontend
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "${FRONTEND_PORT:-80}:80"
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 128M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - rag-network

  # ===========================================================================
  # Ollama Service (Optional - for local LLM inference)
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: agentic-rag-ollama
    restart: unless-stopped
    profiles:
      - ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    deploy:
      resources:
        reservations:
          # Uncomment for GPU support (requires nvidia-docker)
          # devices:
          #   - driver: nvidia
          #     count: all
          #     capabilities: [gpu]
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - rag-network

  # ===========================================================================
  # llama.cpp Server (Optional - for local GGUF model inference)
  # ===========================================================================
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: agentic-rag-llamacpp
    restart: unless-stopped
    profiles:
      - llamacpp
    volumes:
      - llamacpp_models:/models
    ports:
      - "${LLAMACPP_PORT:-8080}:8080"
    command: >
      --host 0.0.0.0
      --port 8080
    deploy:
      resources:
        reservations:
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - rag-network

# =============================================================================
# Networks
# =============================================================================
networks:
  rag-network:
    driver: bridge
    name: agentic-rag-network

# =============================================================================
# Volumes
# =============================================================================
volumes:
  # PostgreSQL data persistence
  postgres_data:
    name: agentic-rag-postgres-data

  # Backend persistent storage
  backend_uploads:
    name: agentic-rag-uploads
  backend_logs:
    name: agentic-rag-logs
  backend_backups:
    name: agentic-rag-backups
  backend_bm25:
    name: agentic-rag-bm25

  # Ollama models
  ollama_data:
    name: agentic-rag-ollama-data

  # llama.cpp GGUF models
  llamacpp_models:
    name: agentic-rag-llamacpp-models
